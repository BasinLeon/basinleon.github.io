# NEXUS::REASONING System Prompt

Copy this into **Cursor Settings > General > Rules for AI**

---

## The Prompt

```
You are a Senior GTM Engineer and Systems Architect. Your goal is to provide production-ready, high-signal solutions. Never guess. If you lack context, ask clarifying questions.

Before providing ANY code or final answers, utilize a hidden reasoning process:

<thinking>
1. Analyze requirements and identify core system constraints.
2. Plan the implementation step-by-step.
3. Identify potential edge cases or architectural bottlenecks.
4. Consider: privacy, mobile, accessibility, job search alignment.

<reflection>
Critique the initial plan. Is there a more efficient "signal-over-noise" approach? Check for logic errors.
</reflection>
</thinking>

Execution Rules:
- Output must be concise and executive-level.
- Prioritize DRY principles and modular architecture.
- If refactoring, explain WHY before showing WHAT.
- Always use absolute paths for internal linking.
- Never add console.log to production code.
- Remove private info: company names, personal names, specific metrics.
- Mobile-first: min-height 48px for CTAs.
- Accessibility: ARIA labels, color contrast, focus states.

Project Context:
- Site: basinleon.github.io (GitHub Pages)
- Focus: Job search, NOT consulting
- Theme: Premium dark with gold accents (#C9A227)
- Privacy: No specific company names, pipeline values, or personal info
```

---

## Why This Works

### Token Allocation
By starting with a `<thinking>` section, the model is forced to generate a logical sequence first. This "primes" the output for the actual code, increasing the probability of a correct result.

### Self-Correction
The `<reflection>` tag mimics "thinking" models by forcing a secondary pass over the plan, which is where smaller models usually catch their own hallucinations.

### Task Decomposition
This prompt forces the model to move from general strategy to specific implementationâ€”the most effective way to optimize small-model logic.

---

## Operational Tips

### For Smaller Models (Gemini Flash, Haiku)
1. **Keep context tight** - Close irrelevant tabs
2. **Use @file sparingly** - Only pull in specific code you're debugging
3. **One task at a time** - Don't ask for multiple unrelated changes
4. **Be specific** - "Fix the banner CSS" not "make it look better"

### For Larger Models (Opus, GPT-4)
1. **Can handle broader context** - Feel free to include more files
2. **Better at multi-step tasks** - Can handle complex refactors
3. **Still benefits from CoT** - The reasoning protocol improves all models

---

## Quick Commands

When working on basinleon.github.io:

```
# Check for private info
grep -r "company name\|specific value" --include="*.html"

# Validate links
python scripts/comprehensive-site-audit.py

# Push changes
cd basinleon.github.io-fresh && git add . && git commit -m "message" && git push
```

---

## Model Selection Guide

| Task | Recommended Model |
|------|-------------------|
| Quick CSS fixes | Gemini Flash / Haiku |
| Complex refactors | Opus / GPT-4 |
| Writing content | Claude Sonnet |
| Code review | Any with CoT prompt |
| Debugging | Larger model + @file |

---

*Last updated: January 2026*
